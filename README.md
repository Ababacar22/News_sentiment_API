# Mon API d'Analyse de Sentiment / My News Sentiment API

Ce projet est une API compl√®te qui scrape le texte d'un article d'actualit√©, analyse son sentiment (positif, n√©gatif, neutre) √† l'aide d'un mod√®le NLP, et renvoie le r√©sultat. Il est enti√®rement conteneuris√© avec Docker.

*This project is a complete API that scrapes the text from a news article, analyzes its sentiment (positive, negative, neutral) using an NLP model, and returns the result. It is fully containerized with Docker.*

---

<details>
<summary><strong>üá´üá∑ Version Fran√ßaise (Cliquez pour voir)</strong></summary>

## üß† Concepts Cl√©s que j'ai appris

Ce projet m'a permis de ma√Ætriser deux concepts fondamentaux :

1.  **Le Pipeline NLP :** J'ai compris comment encha√Æner plusieurs √©tapes de traitement pour transformer un texte non structur√© (un article de blog) en une donn√©e structur√©e (un score de sentiment). Mon pipeline est : `URL -> Requ√™te HTTP -> Parsing HTML -> Extraction de Texte -> Tokenisation -> Inf√©rence du Mod√®le -> JSON (Sentiment)`.
2.  **La Conteneurisation :** J'ai appris pourquoi le "√ßa marche sur ma machine" n'est pas suffisant. En cr√©ant un `Dockerfile`, j'ai "emball√©" mon application, ses d√©pendances Python et m√™me le lourd mod√®le d'IA dans une image portable, reproductible et pr√™te pour le d√©ploiement.

## üõ†Ô∏è Mon Architecture et mes Fonctionnalit√©s

J'ai con√ßu ce service autour de trois piliers principaux, en y ajoutant des fonctionnalit√©s pour le rendre robuste et performant.

### 1. Le Scraper (Data Science)
J'ai utilis√© `requests` pour t√©l√©charger le code HTML d'une URL et `BeautifulSoup4` pour le parser. J'ai impl√©ment√© une logique simple (cibler les balises `<p>`) pour extraire le corps principal du texte, tout en g√©rant les erreurs HTTP et de scraping.

### 2. L'Analyseur de Sentiment (IA / NLP)
C'est le c≈ìur de mon application. J'utilise la biblioth√®que `transformers` de Hugging Face pour charger un mod√®le pr√©-entra√Æn√© (`cardiffnlp/twitter-roberta-base-sentiment`). J'ai fait en sorte que ce mod√®le ne soit charg√© **qu'une seule fois** au d√©marrage de l'API pour garantir des performances optimales.

### 3. L'API (D√©veloppement Logiciel)
J'ai choisi `FastAPI` pour sa rapidit√© et sa simplicit√©.
* Il cr√©e un point de terminaison `POST /analyze` qui accepte une URL.
* J'utilise `Pydantic` pour valider automatiquement les donn√©es d'entr√©e (`HttpUrl`).
* Il g√©n√®re automatiquement une documentation interactive (`/docs`) que j'ai utilis√©e pour tous mes tests.

### ‚ú® Fonctionnalit√©s Avanc√©es
* **Mise en Cache :** J'ai impl√©ment√© un simple cache en m√©moire (un dictionnaire Python) pour stocker les r√©sultats. Si la m√™me URL est demand√©e plusieurs fois, la r√©ponse est instantan√©e (v√©rifiable avec le champ `"from_cache": true`) et √©vite de re-scraper le site.
* **Gestion des Erreurs :** Mon API ne crashe pas. Elle renvoie des codes d'erreur HTTP appropri√©s (422, 404, 503...) si l'URL est invalide, le scraping √©choue ou le mod√®le n'est pas charg√©.

## üíª Technologies Utilis√©es

Pour construire ce projet, je me suis appuy√© sur :

* **Python 3.10**
* **FastAPI** : Pour le framework d'API web.
* **Uvicorn** : Pour servir mon application FastAPI.
* **Hugging Face `transformers`** : Pour le pipeline NLP et le mod√®le RoBERTa.
* **PyTorch** : En tant que backend pour le mod√®le `transformers`.
* **Requests** & **BeautifulSoup4** : Pour le web scraping.
* **Docker** : Pour la conteneurisation.

## üöÄ Comment l'utiliser

Vous pouvez lancer ce projet de deux mani√®res.

### M√©thode 1 : Lancement avec Docker (Recommand√©)

C'est la m√©thode la plus simple, car j'ai d√©j√† tout configur√© dans le `Dockerfile`.

1.  **Assurez-vous que Docker Desktop est en cours d'ex√©cution.**

2.  **Construisez l'image :**
    *(Cette √©tape sera longue la premi√®re fois √† cause de PyTorch)*
    ```bash
    docker build -t sentiment-api .
    ```

3.  **Lancez le conteneur :**
    ```bash
    docker run -p 8000:8000 -it sentiment-api
    ```
    Votre API est maintenant accessible sur `http://127.0.0.1:8000`.

### M√©thode 2 : Lancement Local (D√©veloppement)

1.  **Clonez ce d√©p√¥t :**
    ```bash
    git clone [https://github.com/VOTRE_NOM/VOTRE_REPO.git](https://github.com/VOTRE_NOM/VOTRE_REPO.git)
    cd VOTRE_REPO
    ```

2.  **Cr√©ez un environnement virtuel et activez-le :**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Installez mes d√©pendances :**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Lancez le serveur API :**
    ```bash
    uvicorn main:app --reload
    ```
    Votre API est maintenant accessible sur `http://127.0.0.1:8000`.

## Tester l'API

Une fois l'API lanc√©e (localement ou via Docker), vous avez deux fa√ßons de la tester :

### 1. Avec l'interface web (ma m√©thode pr√©f√©r√©e)

J'ai trouv√© que le plus simple est d'utiliser la documentation auto-g√©n√©r√©e :

1.  Ouvrez votre navigateur et allez sur **http://127.0.0.1:8000/docs**
2.  Cliquez sur le point de terminaison `POST /analyze`.
3.  Cliquez sur "Try it out".
4.  Collez une URL d'article dans le corps de la requ√™te, par exemple :
    ```json
    {
      "url": "[https://www.reuters.com/business/finance/global-markets-wrapup-1-2022-09-21/](https://www.reuters.com/business/finance/global-markets-wrapup-1-2022-09-21/)"
    }
    ```
5.  Cliquez sur "Execute" et voyez le r√©sultat !

### 2. Avec `curl` (depuis le terminal)

```bash
curl -X POST "[http://127.0.0.1:8000/analyze](http://127.0.0.1:8000/analyze)" \
-H "Content-Type: application/json" \
-d '{"url": "[https://www.reuters.com/business/finance/global-markets-wrapup-1-2022-09-21/](https://www.reuters.com/business/finance/global-markets-wrapup-1-2022-09-21/)"}'

### Auteur
Khalifa Ababacar DIALLO
